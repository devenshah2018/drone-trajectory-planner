{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdcb3d0-62af-41df-9fad-fcb84f5af016",
   "metadata": {},
   "source": [
    "# Drone Trajectory Planner\n",
    "\n",
    "In this project, we will develop the drone trajectory planner. This notebook serves as the main file for the project, where we will refer to the instructions and demonstrate our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a63ad4-c249-484a-99e2-d9724aa5d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the files and libraries required for the project\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import copy\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "from src.camera_utils import compute_image_footprint_on_surface, compute_ground_sampling_distance, project_world_point_to_image\n",
    "from src.data_model import Camera, DatasetSpec\n",
    "from src.plan_computation import compute_distance_between_images, compute_speed_during_photo_capture, generate_photo_plan_on_grid, compute_plan_time\n",
    "from src.visualization import plot_photo_plan\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = 'iframe'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731edd83",
   "metadata": {},
   "source": [
    "# Data Specification Model\n",
    "\n",
    "Now, we will model the dataset specification using the following attributes:\n",
    "\n",
    "- Overlap: the ratio (in 0 to 1) of scene shared between two consecutive images.\n",
    "- Sidelap: the ratio (in 0 to 1) of scene shared between two images in adjacent rows.\n",
    "- Height: the height of the scan above the ground (in meters).\n",
    "- Scan_dimension_x: the horizontal size of the rectangle to be scanned (in meters).\n",
    "- Scan_dimension_y: the vertical size of the rectangle to be scanned (in meters).\n",
    "- exposure_time_ms: the exposure time for each image (in milliseconds).\n",
    "\n",
    "In this experiment, we will use a camera with the following specifications:\n",
    "- Overlap: 0.7\n",
    "- Sidelap: 0.7\n",
    "- Height: 30.48 meters (100 feet)\n",
    "- Scan_dimension_x: 150 meters\n",
    "- Scan_dimension_y: 150 meters\n",
    "- exposure_time_ms: 2 milliseconds (1/500 seconds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea66a874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nominal specs: DatasetSpec(overlap=0.7, sidelap=0.7, height=30.48m, scan_area=150x150m, exposure=2ms)\n"
     ]
    }
   ],
   "source": [
    "# Model the nomimal dataset spec\n",
    "\n",
    "overlap = 0.7\n",
    "sidelap = 0.7\n",
    "height = 30.48 # 100 ft\n",
    "scan_dimension_x = 150\n",
    "scan_dimension_y = 150\n",
    "exposure_time_ms = 2 # 1/500 exposure time\n",
    "\n",
    "dataset_spec = DatasetSpec(overlap, sidelap, height, scan_dimension_x, scan_dimension_y, exposure_time_ms)\n",
    "\n",
    "print(f\"Nominal specs: {dataset_spec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65c8b8f-6f33-4cd4-8c3a-74acbdbd2c7d",
   "metadata": {},
   "source": [
    "## Camera Model\n",
    "\n",
    "We want to model the following camera parameters:\n",
    "- focal length along x axis (in pixels)\n",
    "- focal length along y axis (in pixels)\n",
    "- optical center of the image along the x axis (in pixels)\n",
    "- optical center of the image along the y axis (in pixels)\n",
    "- Size of the sensor along the x axis (in mm)\n",
    "- Size of the sensor along the y axis (in mm)\n",
    "- Number of pixels in the image along the x axis\n",
    "- Number of pixels in the image along the y axis\n",
    "\n",
    "In this experiment, we will use a **Skydio VT300L - Wide camera** with the following specifications:\n",
    "- Focal length along x axis: 4938.56 pixels\n",
    "- Focal length along y axis: 4936.49 pixels\n",
    "- Optical center of the image along the x axis: 4095.5 pixels\n",
    "- Optical center of the image along the y axis: 3071.5 pixels\n",
    "- Size of the sensor along the x axis: 13.107 mm\n",
    "- Size of the sensor along the y axis: 9.830 mm\n",
    "- Number of pixels in the image along the x axis: 8192 pixels\n",
    "- Number of pixels in the image along the y axis: 6144 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51f75a3f-b1b1-4087-b9ea-6d5b245ab0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X10 camera model: Camera(fx=4938.56, fy=4936.49, cx=4095.5, cy=3071.5, sensor=13.107x9.830mm, resolution=8192x6144px)\n"
     ]
    }
   ],
   "source": [
    "# Define the parameters for Skydio VT300L - Wide camera\n",
    "# Ref: https://support.skydio.com/hc/en-us/articles/20866347470491-Skydio-X10-camera-and-metadata-overview\n",
    "fx = 4938.56\n",
    "fy = 4936.49\n",
    "cx = 4095.5\n",
    "cy = 3071.5\n",
    "sensor_size_x_mm = 13.107\n",
    "sensor_size_y_mm = 9.830\n",
    "image_size_x = 8192\n",
    "image_size_y = 6144\n",
    "\n",
    "camera_x10 = Camera(fx, fy, cx, cy, sensor_size_x_mm, sensor_size_y_mm, image_size_x, image_size_y)\n",
    "\n",
    "print(f\"X10 camera model: {camera_x10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757018ac",
   "metadata": {},
   "source": [
    "# Camera Operations\n",
    "\n",
    "Up to now, we have modeled the dataset specification and camera parameters. Next, we will implement some utility functions to perform camera operations. These operations include:\n",
    "- Project a 3D world point to an image\n",
    "- Compute image footprint on a surface\n",
    "- Compute the Ground Sampling Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1db4d3-824e-49b9-af3a-dbeb3323625a",
   "metadata": {},
   "source": [
    "## Camera Operation 1: Project 3D world points into the image\n",
    "\n",
    "\n",
    "![Camera Projection](assets/image_projection.png)\n",
    "Reference: [Robert Collins CSE483](https://www.cse.psu.edu/~rtc12/CSE486/lecture12.pdf)\n",
    "\n",
    "\n",
    "| Formula | Description |\n",
    "|---------|------------|\n",
    "| $ x = f_x \\frac{X}{Z} $ | Convert 3D world point `X` to 2D image point `x` |\n",
    "| $ y = f_y \\frac{Y}{Z} $ | Convert 3D world point `Y` to 2D image point `y` |\n",
    "| $ u = x + c_x $ | Convert 2D image point `x` to pixel coordinate `u` |\n",
    "| $ v = y + c_y $ | Convert 2D image point `y` to pixel coordinate `v` |\n",
    "\n",
    "\n",
    "Camera operation `project_world_point_to_image` in `src/camera_utils.py` performs the following operations:\n",
    "1. Takes a 3D world point `[X, Y, Z]` and a camera model as input.\n",
    "2. Projects the 3D world point to 2D image coordinates `[x, y]` using the pinhole camera model.\n",
    "3. Converts the 2D image coordinates `[x, y]` to pixel coordinates `[u, v]` using the camera's optical center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3bd4c0-188b-4f95-98c9-95bb351e2d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25. -30.  50.] projected to [6564.7803   109.60571]\n"
     ]
    }
   ],
   "source": [
    "# Define a 3D world point\n",
    "point_3d = np.array([25, -30, 50], dtype=np.float32)\n",
    "\n",
    "# Expected pixel coordinates after projection\n",
    "expected_uv = np.array([6564.80, 109.60], dtype=np.float32)\n",
    "\n",
    "# Project the 3D world point to image coordinates\n",
    "uv = project_world_point_to_image(camera_x10, point_3d)\n",
    "\n",
    "print(f\"{point_3d} projected to {uv}\")\n",
    "assert np.allclose(uv, expected_uv, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf58acce-ca40-492d-9281-791c05c92776",
   "metadata": {},
   "source": [
    "## Camera Operation 2: Compute Image Footprint on Surface\n",
    "\n",
    "We have written code to *project* a 3D point into the image. The reverse operation is reprojection, where we take $(x, y)$ and compute the $(X, Y)$ for a given value of $Z$. Note that while going from 3D to 2D, the depth becomes ambiguous so we need the to specify the $Z$.\n",
    "\n",
    "An image's footprint is the area on the surface which is captured by the image. We can take the two corners of the image and reproject them at a given distance to obtain the width and length of the image.\n",
    "\n",
    "The camera operation `compute_image_footprint_on_surface` in `src/camera_utils.py` performs the following:\n",
    "1. Takes a camera model and a distance from the surface (in meters) as input.\n",
    "2. Get the pixel coordinates of the two corners of the image: (0, 0) and (image_size_x, image_size_y).\n",
    "3. Reproject the two corners to world coordinates using the existing function `reproject_image_point_to_world`.\n",
    "4. Calculate the footprint dimensions by finding the absolute difference in the X and Y coordinates of the two reprojected corners.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d3e87ae-2123-4362-bf6e-279e6afb360c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint at 100m = [165.87831271 124.46090238]\n",
      "Expected footprint at 100m = [165.88 124.46]\n"
     ]
    }
   ],
   "source": [
    "# Compute image footprint on a surface at 100m distance\n",
    "footprint_at_100m = compute_image_footprint_on_surface(camera_x10, 100)\n",
    "\n",
    "# Expected footprint at 100m distance\n",
    "expected_footprint_at_100m = np.array([165.88, 124.46], dtype=np.float32)\n",
    "\n",
    "print(f\"Footprint at 100m = {footprint_at_100m}\")\n",
    "print(f\"Expected footprint at 100m = {expected_footprint_at_100m}\")\n",
    "assert np.allclose(footprint_at_100m, expected_footprint_at_100m, atol=1e-2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf7af93f-e95b-40e2-b607-39020015381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint at 200m = [331.75662541 248.92180476]\n",
      "Expected footprint at 200m = [331.76 248.92]\n"
     ]
    }
   ],
   "source": [
    "# Compute image footprint on a surface at 200m distance\n",
    "footprint_at_200m = compute_image_footprint_on_surface(camera_x10, 200)\n",
    "\n",
    "# Expected footprint at 200m distance (should be double the footprint at 100m)\n",
    "expected_footprint_at_200m = expected_footprint_at_100m * 2\n",
    "\n",
    "print(f\"Footprint at 200m = {footprint_at_200m}\")\n",
    "print(f\"Expected footprint at 200m = {expected_footprint_at_200m}\")\n",
    "assert np.allclose(footprint_at_200m, expected_footprint_at_200m, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84486ad6-a4c5-4e5a-ba40-ad125ced7d3d",
   "metadata": {},
   "source": [
    "## Camera Operation 3: Ground Sampling Distance (GSD)\n",
    "\n",
    "Ground sampling distance is the length of the ground (in m) captured by a single pixel. This can be calculated from the image footprint (the dimensions of ground captured by the whole sensor) and the number of pixels along the horizontal and vertical dimension.\n",
    "\n",
    "The camera operation `compute_ground_sampling_distance` in `src/camera_utils.py` uses the following formulas:\n",
    "\n",
    "$ GSD_x = \\frac{footprint_x}{image\\_size\\_x} $\n",
    "\n",
    "$ GSD_y = \\frac{footprint_y}{image\\_size\\_y} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f21aa3d1-08be-4bce-8554-ec834ad000bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSD at 100m: 0.020248817469059807\n"
     ]
    }
   ],
   "source": [
    "gsd_at_100m = compute_ground_sampling_distance(camera_x10, 100)\n",
    "expected_gsd_at_100m = 0.0202\n",
    "\n",
    "print(f\"GSD at 100m: {gsd_at_100m}\")\n",
    "\n",
    "assert np.allclose(gsd_at_100m, expected_gsd_at_100m, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47477599-e78b-46d4-9cac-725bcbb5e6eb",
   "metadata": {},
   "source": [
    "## Reprojection from 2D to 3D\n",
    "\n",
    "Reprojection is the reverse of projection. We take a 2D image point and a depth value (Z) and compute the corresponding 3D world point (X, Y, Z).\n",
    "\n",
    "The camera operation `reproject_image_point_to_world` in `src/camera_utils.py` performs the following:\n",
    "1. Takes a 2D image point `[u, v]`, a camera model, and a distance from the surface (depth) as input.\n",
    "2. Converts the pixel coordinates `[u, v]` to normalized image coordinates `[x, y]` using the camera's optical center.\n",
    "3. Reprojects the normalized image coordinates `[x, y]` to world coordinates `[X, Y, Z]` using the pinhole camera model and the given depth.\n",
    "\n",
    "Note: There is an ambiguity in depth when going from 2D to 3D we will address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11406c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original 3D point: [ 25. -30.  50.]\n",
      "Projected to 2D: [6564.7803   109.60571]\n",
      "Reprojected back to 3D: [ 25.000004 -30.000002  50.      ]\n",
      "\n",
      "Difference between original and reprojected: [3.8146973e-06 1.9073486e-06 0.0000000e+00]\n"
     ]
    }
   ],
   "source": [
    "from src.camera_utils import reproject_image_point_to_world\n",
    "\n",
    "original_3d_point = np.array([25, -30, 50], dtype=np.float32)\n",
    "print(f\"Original 3D point: {original_3d_point}\")\n",
    "\n",
    "projected_2d = project_world_point_to_image(camera_x10, original_3d_point)\n",
    "print(f\"Projected to 2D: {projected_2d}\")\n",
    "\n",
    "reprojected_3d = reproject_image_point_to_world(camera_x10, projected_2d, original_3d_point[2])\n",
    "print(f\"Reprojected back to 3D: {reprojected_3d}\")\n",
    "\n",
    "print(f\"\\nDifference between original and reprojected: {np.abs(original_3d_point - reprojected_3d)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceda1fa",
   "metadata": {},
   "source": [
    "### Understanding the Ambiguity in 2D to 3D Reprojection\n",
    "\n",
    "To go from 2D back to 3D, we **need additional information** - specifically the depth (Z coordinate). This is because:\n",
    "\n",
    "1. **The projection from 3D to 2D loses depth information** - multiple 3D points along the same ray from the camera center will project to the same 2D pixel.\n",
    "\n",
    "2. **Without depth, the problem is under-constrained** - given just a 2D pixel coordinate (u, v), there are infinite possible 3D points that could have projected to that pixel.\n",
    "\n",
    "3. **The depth provides the missing constraint** - once we know the Z distance, we can uniquely determine the X and Y coordinates.\n",
    "\n",
    "Let's demonstrate this by showing how different depths give different 3D points for the same 2D pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50725f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image point: [4000 3000]\n",
      "At depth 50m: X=-0.97, Y=-0.72, Z=50.00\n",
      "  Projects back to: [4000.0, 3000.0]\n",
      "At depth 100m: X=-1.93, Y=-1.45, Z=100.00\n",
      "  Projects back to: [4000.0, 3000.0]\n",
      "At depth 200m: X=-3.87, Y=-2.90, Z=200.00\n",
      "  Projects back to: [4000.0, 3000.0]\n",
      "At depth 500m: X=-9.67, Y=-7.24, Z=500.00\n",
      "  Projects back to: [4000.0, 3000.0]\n"
     ]
    }
   ],
   "source": [
    "image_point = np.array([4000, 3000])\n",
    "print(f\"Image point: {image_point}\")\n",
    "\n",
    "for depth in [50, 100, 200, 500]:\n",
    "    world_point = reproject_image_point_to_world(camera_x10, image_point, depth)\n",
    "    print(f\"At depth {depth}m: X={world_point[0]:.2f}, Y={world_point[1]:.2f}, Z={world_point[2]:.2f}\")\n",
    "    reprojected_2d = project_world_point_to_image(camera_x10, world_point)\n",
    "    print(f\"  Projects back to: [{reprojected_2d[0]:.1f}, {reprojected_2d[1]:.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135a5201-bcbc-4e57-9201-31d3dbb50005",
   "metadata": {},
   "source": [
    "# Compute the Distance Between Photos\n",
    "\n",
    "The overlap and sidelap are the ratio of the dimensions shared between two photos. We already know the footprint of a single image at a given distance. Can we convert the ratio into actual distances? And how does the distance on the surface relate to distance travelled by the camera?\n",
    "\n",
    "The function `compute_distance_between_images` in `src/plan_computation.py` performs the following:\n",
    "1. Take a camera model and dataset specification as input.\n",
    "2. Validate that the overlap and sidelap are in the range [0, 1).\n",
    "3. Compute the footprint of a single image on the surface at the specified height using the existing function `compute_image_footprint_on_surface`.\n",
    "4. Calculate the distance between image centers in the horizontal and vertical directions using the formulas:\n",
    "   - Distance_x = footprint_x * (1 - overlap)\n",
    "   - Distance_y = footprint_y * (1 - sidelap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70b32a91-2577-46ec-bb07-9a0a5fd01243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed distance for X10 camera with nominal dataset specs: [15.16791291 11.38070491]\n"
     ]
    }
   ],
   "source": [
    "# Computed distance between images\n",
    "computed_distances = compute_distance_between_images(camera_x10, dataset_spec)\n",
    "\n",
    "# Expected distances between images for the nominal dataset spec\n",
    "expected_distances = np.array([15.17, 11.38], dtype=np.float32)\n",
    "\n",
    "print(f\"Computed distance for X10 camera with nominal dataset specs: {computed_distances}\")\n",
    "assert np.allclose(computed_distances, expected_distances, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5474e7ac-28bb-4ab3-ad8d-cb5398d5696b",
   "metadata": {},
   "source": [
    "## Experimenting with Photo Spacing\n",
    "\n",
    "Now, we are going to configure the following parameters and assess how these affect the computed distances:\n",
    "- Data specification height\n",
    "- Camera focal length\n",
    "- Image size\n",
    "\n",
    "Adjusting flight height, camera focal length, and image resolution directly impacts photo spacing. Pilots can use these relationships to optimize flight plans for coverage, resolution, and efficiency.\n",
    "\n",
    "### Baseline Case\n",
    "\n",
    "- **Parameters:**  \n",
    "    - Camera: `camera_x10`  \n",
    "    - Dataset Spec: `dataset_spec` (height=30.48m, overlap=0.7, sidelap=0.7)\n",
    "- **Result:**  \n",
    "    - Baseline distances:  \n",
    "        - dx = 15.17 m  \n",
    "        - dy = 11.38 m  \n",
    "\n",
    "The spacing is determined by the camera's ground footprint and the required overlap/sidelap. Larger footprints or lower overlap mean greater spacing.\n",
    "\n",
    "### Doubling Flight Height\n",
    "\n",
    "- **Change:**  \n",
    "    - Height increased from 30.48m to 60.96m.\n",
    "- **Result:** Distances roughly double\n",
    "    - dx ≈ 30.34 m  \n",
    "    - dy ≈ 22.76 m  \n",
    "    - Ratio to baseline ≈ 2x\n",
    "\n",
    "Increasing flight height increases the ground area captured by each photo (footprint scales linearly with height). Thus, the drone can space photos further apart. Pilots can cover larger areas faster at higher altitudes, but ground resolution decreases.\n",
    "\n",
    "### Halving Focal Length\n",
    "\n",
    "- **Change:**  \n",
    "    - Focal length (fx, fy) reduced by half.\n",
    "- **Result:** Distances roughly double\n",
    "    - dx ≈ 30.34 m  \n",
    "    - dy ≈ 22.76 m  \n",
    "    - Ratio to baseline ≈ 2x\n",
    "\n",
    "A shorter focal length makes the camera \"wider,\" increasing the ground footprint. Photos can be spaced further apart. Wider lenses allow faster coverage but reduce image detail.\n",
    "\n",
    "### Halving Image Size\n",
    "\n",
    "- **Change:** Image size (pixels) halved in both dimensions.\n",
    "- **Result:** Distances roughly halve\n",
    "    - dx ≈ 7.58 m  \n",
    "    - dy ≈ 5.69 m  \n",
    "    - Ratio to baseline ≈ 0.5x\n",
    "\n",
    "Lower image size means each photo covers less ground, so photos must be taken closer together to maintain coverage. Lowering size saves storage but requires denser flight patterns and more photos.\n",
    "\n",
    "### Summary\n",
    "\n",
    "| Change                | dx (m)   | dy (m)   | Ratio to Baseline | Analysis                                                   |\n",
    "|-----------------------|----------|----------|-------------------|------------------------------------------------------------|\n",
    "| Baseline              | 15.17    | 11.38    | 1x                | Determined by footprint and overlap/sidelap                |\n",
    "| Double Height         | 30.34    | 22.76    | ~2x               | Higher altitude → larger footprint → greater spacing       |\n",
    "| Halve Focal Length    | 30.34    | 22.76    | ~2x               | Wider lens → larger footprint → greater spacing            |\n",
    "| Halve Image Size      | 7.58     | 5.69     | ~0.5x             | Lower resolution → smaller footprint → closer spacing      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7f9334a-6ee1-4aa4-8770-d0f48b36f18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline dataset_spec: DatasetSpec(overlap=0.7, sidelap=0.7, height=30.48m, scan_area=150x150m, exposure=2ms)\n",
      "\n",
      "Baseline distances (m) [dx, dy]: [15.16791291 11.38070491]\n",
      "\n",
      "Double height\n",
      " distances: [30.33582583 22.76140983]  ratio to baseline: [2. 2.]\n",
      "\n",
      "Halve focal length (fx,fy * 0.5)\n",
      " distances: [30.33582583 22.76140983]  ratio to baseline: [2. 2.]\n",
      "\n",
      "Half image size (pixels)\n",
      " distances: [7.58395646 5.69035246]  ratio to baseline: [0.5 0.5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Baseline dataset_spec:\", dataset_spec)\n",
    "baseline_distances = compute_distance_between_images(camera_x10, dataset_spec)\n",
    "print(\"\\nBaseline distances (m) [dx, dy]:\", baseline_distances)\n",
    "\n",
    "# Double height\n",
    "spec_height = copy.copy(dataset_spec)\n",
    "spec_height.height = dataset_spec.height * 2.0\n",
    "dist_height = compute_distance_between_images(camera_x10, spec_height)\n",
    "print(\"\\nDouble height\")\n",
    "print(\" distances:\", dist_height, \" ratio to baseline:\", dist_height / baseline_distances)\n",
    "\n",
    "# Halve focal length\n",
    "cam_half_fx = copy.copy(camera_x10)\n",
    "cam_half_fx.fx *= 0.5\n",
    "cam_half_fx.fy *= 0.5\n",
    "dist_half_fx = compute_distance_between_images(cam_half_fx, dataset_spec)\n",
    "print(\"\\nHalve focal length (fx,fy * 0.5)\")\n",
    "print(\" distances:\", dist_half_fx, \" ratio to baseline:\", dist_half_fx / baseline_distances)\n",
    "\n",
    "# Halve image size\n",
    "cam_half_res = copy.copy(camera_x10)\n",
    "cam_half_res.image_size_x //= 2\n",
    "cam_half_res.image_size_y //= 2\n",
    "dist_half_res = compute_distance_between_images(cam_half_res, dataset_spec)\n",
    "print(\"\\nHalf image size (pixels)\")\n",
    "print(\" distances:\", dist_half_res, \" ratio to baseline:\", dist_half_res / baseline_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea6d39a-89af-42bc-bb63-2d22380dec1f",
   "metadata": {},
   "source": [
    "## Non-Nadir photos\n",
    "\n",
    "We have solved for the distance assuming that the camera is facing straight down to the ground. This is called [Nadir scanning](https://support.esri.com/en-us/gis-dictionary/nadir). However, in practice we might want a custom gimbal angle.\n",
    "\n",
    "We will introduce a double `camera_angle` parameter (which is the angle from the X-axis) in the dataset specification and work out how to adapt the computation.\n",
    "\n",
    "![Non Nadir Footprint](assets/non_nadir_gimbal_angle.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f80b101d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footprint at 30.48m with 30° tilt: [50.55970971 43.80435364]\n",
      "Distance between images (non-nadir, 30° tilt): [15.16791291 13.14130609]\n"
     ]
    }
   ],
   "source": [
    "from src.plan_computation import compute_non_nadir_footprint, compute_distance_between_images_non_nadir\n",
    "\n",
    "# Define a camera angle (in degrees) for non-nadir computation\n",
    "camera_angle_deg = 30\n",
    "camera_angle_rad = np.deg2rad(camera_angle_deg)\n",
    "\n",
    "# Compute for 30 degree tilt\n",
    "footprint_non_nadir = compute_non_nadir_footprint(camera_x10, dataset_spec.height, camera_angle_rad)\n",
    "print(f\"Footprint at {dataset_spec.height}m with {camera_angle_deg}° tilt: {footprint_non_nadir}\")\n",
    "\n",
    "# Compute the distance between images using the new footprint\n",
    "distances_non_nadir = compute_distance_between_images_non_nadir(camera_x10, dataset_spec, camera_angle_rad)\n",
    "print(f\"Distance between images (non-nadir, {camera_angle_deg}° tilt): {distances_non_nadir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfc13a5-d13d-4930-a47b-2851e5458ceb",
   "metadata": {},
   "source": [
    "# Compute the Maximum Speed For Blur Free Photos\n",
    "\n",
    "To restrict motion blur due to camera movement to tolerable limits, we need to restrict the speed such that the image contents move less than 1px away. \n",
    "\n",
    "The ground sampling distance (GSD) tells us how much ground is covered by a single pixel. From the previous week, we know that this is the maximum movement the camera can have (distance). The speed is then distance divided by time, which is provided by the data model.\n",
    "\n",
    "The function `compute_speed_during_photo_capture` in `src/plan_computation.py` performs the following operations:\n",
    "1. Takes a camera model, dataset specification, and allowed movement in pixels as input.\n",
    "2. Computes the ground sampling distance (GSD) at the specified flight height using the existing function `compute_ground_sampling_distance`.\n",
    "3. Calculates the maximum allowed ground movement during the exposure time as `allowed_movement_px * GSD`.\n",
    "4. Converts the exposure time from milliseconds to seconds.\n",
    "5. Computes the maximum speed as `max_movement / exposure_time_s`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1292957c-d66f-4ac9-bb6f-1dc783d1a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed speed during photo captures: 3.09\n",
      "Expected speed: 3.09\n"
     ]
    }
   ],
   "source": [
    "# Compute the maximum speed during photo capture to avoid motion blur\n",
    "computed_speed = compute_speed_during_photo_capture(camera_x10, dataset_spec, allowed_movement_px=1)\n",
    "\n",
    "# Expected speed based on manual calculation\n",
    "expected_speed = 3.09\n",
    "\n",
    "print(f\"Computed speed during photo captures: {computed_speed:.2f}\")\n",
    "print(f\"Expected speed: {expected_speed:.2f}\")\n",
    "\n",
    "assert np.allclose(computed_speed, expected_speed, atol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fe63d9-c416-4bbf-b0ce-8c678de681de",
   "metadata": {},
   "source": [
    "## Experimenting with Speed Calculation\n",
    "\n",
    "We are going to configure the following parameters and assess how these affect the computed speed:\n",
    "- Data specification height\n",
    "- Camera focal length\n",
    "- Image size\n",
    "\n",
    "Adjusting flight height, camera focal length, and image resolution directly impacts the maximum allowable speed for blur-free photos. Pilots can use these relationships to optimize flight plans for speed and image quality.\n",
    "\n",
    "### Baseline Case\n",
    "\n",
    "- **Parameters:**  \n",
    "    - Camera: `camera_x10`  \n",
    "    - Dataset Spec: `dataset_spec` (height=30.48m, overlap=0.7, sidelap=0.7)\n",
    "- **Result:**  \n",
    "    - Computed Speed: 3.09 m/s  \n",
    "\n",
    "### Doubling Flight Height\n",
    "\n",
    "- **Change:**  \n",
    "    - Height increased from 30.48m to 60.96m.\n",
    "- **Result:** Speed roughly doubles\n",
    "    - speed ≈ 6.17 m/s\n",
    "\n",
    "Increasing flight height increases the Ground Sampling Distance (GSD). Since the maximum allowable speed is limited by how much the scene can move during exposure (typically 1 pixel), a larger GSD means the drone can move faster while staying within the same pixel movement limit. Pilots can fly faster at higher altitudes while maintaining sharp images, but this comes at the cost of reduced ground resolution.\n",
    "\n",
    "### Halving Focal Length\n",
    "\n",
    "- **Change:**  \n",
    "    - Focal length (fx, fy) reduced by half.\n",
    "- **Result:** Speed roughly doubles\n",
    "    - speed ≈ 6.27 m/s\n",
    "\n",
    "A shorter focal length makes the camera \"wider,\" increasing the Ground Sampling Distance (GSD). Same as the previous experiment, a larger GSD means the drone can move faster while staying within the same pixel movement limit. Pilots can fly faster with wider lenses while maintaining sharp images, though this comes at the cost of reduced image detail per pixel.\n",
    "\n",
    "### Halving Image Size\n",
    "\n",
    "- **Change:** Image size (pixels) halved in both dimensions.\n",
    "- **Result:** Speed stays the same\n",
    "    - speed ≈ 3.09 m/s\n",
    "\n",
    "Lower image size (fewer pixels) doesn't change the Ground Sampling Distance (GSD) because GSD depends on the physical sensor size and focal length, not the number of pixels. Since the maximum allowable speed is determined by GSD and exposure time, halving the pixel count doesn't affect the blur-free speed limit. However, lower resolution means less detail captured per image, requiring pilots to potentially fly lower or use longer exposures for the same level of detail.\n",
    "\n",
    "### Summary\n",
    "\n",
    "\n",
    "| Change                | Speed (m/s) | Ratio to Baseline | Analysis                                                   |\n",
    "|-----------------------|-------------|-------------------|------------------------------------------------------------|\n",
    "| Baseline              | 3.09        | 1x                | Determined by GSD and exposure time                        |\n",
    "| Double Height         | 6.17        | ~2x               | Higher altitude → larger GSD → higher allowable speed      |\n",
    "| Halve Focal Length    | 6.17        | ~2x               | Wider lens → larger GSD → higher allowable speed           |\n",
    "| Halve Image Size      | 3.09        | ~1x               | Pixel count doesn't affect GSD → no speed change           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85212946-83d0-475c-9740-e04d10a1ec32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed distance: 3.09\n",
      "\n",
      "Double height\n",
      " speed: 6.171839564569429  ratio to baseline: 2.0\n",
      "\n",
      "Halve focal length (fx,fy * 0.5)\n",
      " speed: 6.171839564569429  ratio to baseline: 2.0\n",
      "\n",
      "Half image size (pixels)\n",
      " speed: 3.0859197822847144  ratio to baseline: 1.0\n"
     ]
    }
   ],
   "source": [
    "camera_ = copy.copy(camera_x10)\n",
    "dataset_spec_ = copy.copy(dataset_spec)\n",
    "\n",
    "computed_speed_ = compute_speed_during_photo_capture(camera_, dataset_spec_)\n",
    "print(f\"Computed distance: {computed_speed_:.2f}\")\n",
    "\n",
    "# Double height\n",
    "spec_height = copy.copy(dataset_spec)\n",
    "spec_height.height = dataset_spec.height * 2.0\n",
    "speed_height = compute_speed_during_photo_capture(camera_x10, spec_height)\n",
    "print(\"\\nDouble height\")\n",
    "print(\" speed:\", speed_height, \" ratio to baseline:\", speed_height / computed_speed_)\n",
    "\n",
    "# Halve focal length\n",
    "cam_half_fx = copy.copy(camera_x10)\n",
    "cam_half_fx.fx *= 0.5\n",
    "cam_half_fx.fy *= 0.5\n",
    "speed_half_fx = compute_speed_during_photo_capture(cam_half_fx, dataset_spec)\n",
    "print(\"\\nHalve focal length (fx,fy * 0.5)\")\n",
    "print(\" speed:\", speed_half_fx, \" ratio to baseline:\", speed_half_fx / computed_speed_)\n",
    "\n",
    "# Halve image size\n",
    "cam_half_res = copy.copy(camera_x10)\n",
    "cam_half_res.image_size_x //= 2\n",
    "cam_half_res.image_size_y //= 2\n",
    "speed_half_res = compute_speed_during_photo_capture(cam_half_res, dataset_spec)\n",
    "print(\"\\nHalf image size (pixels)\")\n",
    "print(\" speed:\", speed_half_res, \" ratio to baseline:\", speed_half_res / computed_speed_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d7075d-f520-43bd-8b8b-b36e9c6a6bc2",
   "metadata": {},
   "source": [
    "# Week 6: Generate Full Flight Plans  \n",
    "\n",
    "We now have all the tools to generate the full flight plan.\n",
    "\n",
    "Steps for this week:\n",
    "1. Define the `Waypoint` data model. What attributes should the data model have?\n",
    "   1. For Nadir scans, just the position of the camera is enough as we will always look drown to the ground.\n",
    "   2. For general case (bonus), we also need to define where the drone will look at.\n",
    "3. Implement the function `generate_photo_plan_on_grid` to generate the full plan.\n",
    "   1. Compute the maximum distance between two images, horizontally and vertically.\n",
    "   2. Layer the images such that we cover the whole scan area. Note that you need to take care when the scan dimension is not a multiple of distance between images. Example: to cover 45m length with 10m between images, we would need 4.5 images. Not possible. 4 images would not satisfy the overlap, so we should go with 5. How should we arrange 5 images in the given 45m.\n",
    "   3. Assign the speed to each waypoint.\n",
    "\n",
    "$\\color{red}{\\text{TODO: }}$ Implement:\n",
    "- `Waypoint` in `src/data_model.py`\n",
    "- `generate_photo_plan_on_grid` in `src/plan_computation.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f1e5c3e-9974-4f50-9413-1b9c5b523051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed plan with 165 waypoints\n"
     ]
    }
   ],
   "source": [
    "computed_plan = generate_photo_plan_on_grid(camera_x10, dataset_spec) \n",
    "\n",
    "print(f\"Computed plan with {len(computed_plan)} waypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71f3bf15-c584-44da-a789-c821acb3f9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx 0: Waypoint(pos=(-75.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 1: Waypoint(pos=(-60.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 2: Waypoint(pos=(-45.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 3: Waypoint(pos=(-30.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 4: Waypoint(pos=(-15.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 5: Waypoint(pos=(0.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 6: Waypoint(pos=(15.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 7: Waypoint(pos=(30.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 8: Waypoint(pos=(45.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 9: Waypoint(pos=(60.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 10: Waypoint(pos=(75.0, -75.0, 30.5), speed=3.09m/s)\n",
      "Idx 11: Waypoint(pos=(75.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 12: Waypoint(pos=(60.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 13: Waypoint(pos=(45.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 14: Waypoint(pos=(30.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 15: Waypoint(pos=(15.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 16: Waypoint(pos=(0.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 17: Waypoint(pos=(-15.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 18: Waypoint(pos=(-30.0, -64.3, 30.5), speed=3.09m/s)\n",
      "Idx 19: Waypoint(pos=(-45.0, -64.3, 30.5), speed=3.09m/s)\n"
     ]
    }
   ],
   "source": [
    "MAX_NUM_WAYPOINTS_TO_PRINT = 20\n",
    "\n",
    "for idx, waypoint in enumerate(computed_plan[:MAX_NUM_WAYPOINTS_TO_PRINT]):\n",
    "    print(f\"Idx {idx}: {waypoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5544587f-fff7-406a-be65-ad99c7217e51",
   "metadata": {},
   "source": [
    "## Time Computation\n",
    "\n",
    "**Note**\n",
    "- **Max drone speed:** 16 m/s  \n",
    "- **Max acceleration:** 3.5 m/s²  \n",
    "- **v0:** starting speed (here, v_photo)\n",
    "\n",
    "**Triangular profile**\n",
    "\n",
    "Occurs when the segment is too short to reach cruise (max) speed. The drone accelerates from v0 to a peak v_peak (< v_max) and then symmetrically decelerates back to v0.\n",
    "\n",
    "Key relations (symmetric accel/decel, |a| = a_max):\n",
    "- Peak speed:\n",
    "    $v_{\\text{peak}}=\\sqrt{v_0^2 + a_{\\max}\\cdot d}$\n",
    "\n",
    "- Acceleration time:\n",
    "    $t_{\\text{acc}}=\\frac{v_{\\text{peak}}-v_0}{a_{\\max}}$\n",
    "\n",
    "- Total travel time:\n",
    "    $t_{\\text{total}}=2\\,t_{\\text{acc}}$\n",
    "\n",
    "Use this when the available distance d is too short to accelerate to v_max and then decelerate.\n",
    "\n",
    "**Trapezoidal profile**\n",
    "\n",
    "Occurs when the segment is long enough to reach v_max. The speed profile: accelerate from v0 → v_max, cruise at v_max, then decelerate back to v0.\n",
    "\n",
    "Distances and times:\n",
    "- Distance for accel+decel:\n",
    "    $d_{\\text{acc+dec}}=\\frac{v_{\\max}^2 - v_0^2}{a_{\\max}}$\n",
    "\n",
    "- Cruise distance:\n",
    "    $d_{\\text{cruise}}=d - d_{\\text{acc+dec}}$\n",
    "\n",
    "- Acceleration time:\n",
    "    $t_{\\text{acc}}=\\frac{v_{\\max}-v_0}{a_{\\max}}$\n",
    "\n",
    "- Cruise time:\n",
    "    $t_{\\text{cruise}}=\\frac{d_{\\text{cruise}}}{v_{\\max}}$\n",
    "\n",
    "- Total travel time:\n",
    "    $t_{\\text{total}}=2\\,t_{\\text{acc}}+t_{\\text{cruise}}$\n",
    "\n",
    " **Decision rule (simple algorithm)**\n",
    "1. Compute $v_{\\text{peak}}=\\sqrt{v_0^2 + a_{\\max}\\cdot d}$  \n",
    "2. If $v_{\\text{peak}}\\le v_{\\max}$ → use **triangular** profile (peak = v_peak).  \n",
    "3. Else → use **trapezoidal** profile (accelerate to v_max, cruise, decelerate).\n",
    "\n",
    "**Why it matters for the planner**\n",
    "- Provides the minimum feasible travel time while ensuring the drone can decelerate to v_photo before capture.\n",
    "- Affects mission duration, energy usage, and safety (braking capability).\n",
    "- Practical use: compute per-segment profile (triangular vs trapezoidal) using (v0 = v_photo, v_max, a_max) to obtain accurate travel times and ensure safe deceleration to capture speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b23d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated total mission time: 441.0 s (7.4 min)\n",
      "Number of segments: 164\n",
      "Segment 0: {'index': 0, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 1: {'index': 1, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 2: {'index': 2, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 3: {'index': 3, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 4: {'index': 4, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 5: {'index': 5, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 6: {'index': 6, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 7: {'index': 7, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 8: {'index': 8, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 9: {'index': 9, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 10: {'index': 10, 'distance': 10.714285714285708, 'travel_time_s': 2.1550884769710774, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841, 't_acc': 1.0775442384855387}, 'capture_time_s': 0.002}\n",
      "Segment 11: {'index': 11, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 12: {'index': 12, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 13: {'index': 13, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 14: {'index': 14, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 15: {'index': 15, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 16: {'index': 16, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 17: {'index': 17, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 18: {'index': 18, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 19: {'index': 19, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 20: {'index': 20, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 21: {'index': 21, 'distance': 10.714285714285722, 'travel_time_s': 2.1550884769710787, 'profile': {'type': 'triangular', 'v_peak': 6.857324616984102, 't_acc': 1.0775442384855394}, 'capture_time_s': 0.002}\n",
      "Segment 22: {'index': 22, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 23: {'index': 23, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 24: {'index': 24, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 25: {'index': 25, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 26: {'index': 26, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 27: {'index': 27, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 28: {'index': 28, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 29: {'index': 29, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 30: {'index': 30, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 31: {'index': 31, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 32: {'index': 32, 'distance': 10.714285714285708, 'travel_time_s': 2.1550884769710774, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841, 't_acc': 1.0775442384855387}, 'capture_time_s': 0.002}\n",
      "Segment 33: {'index': 33, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 34: {'index': 34, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 35: {'index': 35, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 36: {'index': 36, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 37: {'index': 37, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 38: {'index': 38, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 39: {'index': 39, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 40: {'index': 40, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 41: {'index': 41, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 42: {'index': 42, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 43: {'index': 43, 'distance': 10.714285714285715, 'travel_time_s': 2.155088476971078, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841005, 't_acc': 1.077544238485539}, 'capture_time_s': 0.002}\n",
      "Segment 44: {'index': 44, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 45: {'index': 45, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 46: {'index': 46, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 47: {'index': 47, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 48: {'index': 48, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 49: {'index': 49, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 50: {'index': 50, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 51: {'index': 51, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 52: {'index': 52, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 53: {'index': 53, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 54: {'index': 54, 'distance': 10.714285714285715, 'travel_time_s': 2.155088476971078, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841005, 't_acc': 1.077544238485539}, 'capture_time_s': 0.002}\n",
      "Segment 55: {'index': 55, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 56: {'index': 56, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 57: {'index': 57, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 58: {'index': 58, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 59: {'index': 59, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 60: {'index': 60, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 61: {'index': 61, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 62: {'index': 62, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 63: {'index': 63, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 64: {'index': 64, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 65: {'index': 65, 'distance': 10.714285714285708, 'travel_time_s': 2.1550884769710774, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841, 't_acc': 1.0775442384855387}, 'capture_time_s': 0.002}\n",
      "Segment 66: {'index': 66, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 67: {'index': 67, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 68: {'index': 68, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 69: {'index': 69, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 70: {'index': 70, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 71: {'index': 71, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 72: {'index': 72, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 73: {'index': 73, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 74: {'index': 74, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 75: {'index': 75, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 76: {'index': 76, 'distance': 10.714285714285722, 'travel_time_s': 2.1550884769710787, 'profile': {'type': 'triangular', 'v_peak': 6.857324616984102, 't_acc': 1.0775442384855394}, 'capture_time_s': 0.002}\n",
      "Segment 77: {'index': 77, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 78: {'index': 78, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 79: {'index': 79, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 80: {'index': 80, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 81: {'index': 81, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 82: {'index': 82, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 83: {'index': 83, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 84: {'index': 84, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 85: {'index': 85, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 86: {'index': 86, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 87: {'index': 87, 'distance': 10.714285714285708, 'travel_time_s': 2.1550884769710774, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841, 't_acc': 1.0775442384855387}, 'capture_time_s': 0.002}\n",
      "Segment 88: {'index': 88, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 89: {'index': 89, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 90: {'index': 90, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 91: {'index': 91, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 92: {'index': 92, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 93: {'index': 93, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 94: {'index': 94, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 95: {'index': 95, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 96: {'index': 96, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 97: {'index': 97, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 98: {'index': 98, 'distance': 10.714285714285708, 'travel_time_s': 2.1550884769710774, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841, 't_acc': 1.0775442384855387}, 'capture_time_s': 0.002}\n",
      "Segment 99: {'index': 99, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 100: {'index': 100, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 101: {'index': 101, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 102: {'index': 102, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 103: {'index': 103, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 104: {'index': 104, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 105: {'index': 105, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 106: {'index': 106, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 107: {'index': 107, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 108: {'index': 108, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 109: {'index': 109, 'distance': 10.714285714285722, 'travel_time_s': 2.1550884769710787, 'profile': {'type': 'triangular', 'v_peak': 6.857324616984102, 't_acc': 1.0775442384855394}, 'capture_time_s': 0.002}\n",
      "Segment 110: {'index': 110, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 111: {'index': 111, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 112: {'index': 112, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 113: {'index': 113, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 114: {'index': 114, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 115: {'index': 115, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 116: {'index': 116, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 117: {'index': 117, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 118: {'index': 118, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 119: {'index': 119, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 120: {'index': 120, 'distance': 10.714285714285708, 'travel_time_s': 2.1550884769710774, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841, 't_acc': 1.0775442384855387}, 'capture_time_s': 0.002}\n",
      "Segment 121: {'index': 121, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 122: {'index': 122, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 123: {'index': 123, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 124: {'index': 124, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 125: {'index': 125, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 126: {'index': 126, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 127: {'index': 127, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 128: {'index': 128, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 129: {'index': 129, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 130: {'index': 130, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 131: {'index': 131, 'distance': 10.714285714285708, 'travel_time_s': 2.1550884769710774, 'profile': {'type': 'triangular', 'v_peak': 6.8573246169841, 't_acc': 1.0775442384855387}, 'capture_time_s': 0.002}\n",
      "Segment 132: {'index': 132, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 133: {'index': 133, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 134: {'index': 134, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 135: {'index': 135, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 136: {'index': 136, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 137: {'index': 137, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 138: {'index': 138, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 139: {'index': 139, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 140: {'index': 140, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 141: {'index': 141, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 142: {'index': 142, 'distance': 10.714285714285722, 'travel_time_s': 2.1550884769710787, 'profile': {'type': 'triangular', 'v_peak': 6.857324616984102, 't_acc': 1.0775442384855394}, 'capture_time_s': 0.002}\n",
      "Segment 143: {'index': 143, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 144: {'index': 144, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 145: {'index': 145, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 146: {'index': 146, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 147: {'index': 147, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 148: {'index': 148, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 149: {'index': 149, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 150: {'index': 150, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 151: {'index': 151, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 152: {'index': 152, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 153: {'index': 153, 'distance': 10.714285714285722, 'travel_time_s': 2.1550884769710787, 'profile': {'type': 'triangular', 'v_peak': 6.857324616984102, 't_acc': 1.0775442384855394}, 'capture_time_s': 0.002}\n",
      "Segment 154: {'index': 154, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 155: {'index': 155, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 156: {'index': 156, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 157: {'index': 157, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 158: {'index': 158, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 159: {'index': 159, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 160: {'index': 160, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 161: {'index': 161, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 162: {'index': 162, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n",
      "Segment 163: {'index': 163, 'distance': 15.0, 'travel_time_s': 2.7368812378527148, 'profile': {'type': 'triangular', 'v_peak': 7.875461948526965, 't_acc': 1.3684406189263574}, 'capture_time_s': 0.002}\n"
     ]
    }
   ],
   "source": [
    "from src.plan_computation import compute_plan_time, compute_speed_during_photo_capture\n",
    "\n",
    "v_photo = compute_speed_during_photo_capture(camera_x10, dataset_spec, allowed_movement_px=1)\n",
    "positions = [np.array([wp.x, wp.y, wp.z]) for wp in computed_plan]\n",
    "exposure_s = dataset_spec.exposure_time_ms / 1000.0\n",
    "\n",
    "total_time_s, segments = compute_plan_time(positions, v_photo, exposure_time_s=exposure_s)\n",
    "print(f\"Estimated total mission time: {total_time_s:.1f} s ({total_time_s/60:.1f} min)\")\n",
    "print(\"Number of segments:\", len(segments))\n",
    "for idx, segment in enumerate(segments):\n",
    "    print(f\"Segment {idx}: {segment}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a326c0e-86ff-443d-98ee-7a4fae0bfe81",
   "metadata": {},
   "source": [
    "# Week 7: Visualize Flight Plans\n",
    "\n",
    "This week, we will use a third party plotting framework called [Plotly](https://plotly.com/python/) to visualize our plans. Please follow this [tutorial](https://www.kaggle.com/code/kanncaa1/plotly-tutorial-for-beginners) to gain some basic experience with Plotly, and then come up with your own visualization function. You are free to choose to come up with your own visualization, and use something other than Plotly.\n",
    "\n",
    "$\\color{red}{\\text{TODO: }}$ Implement `plot_photo_plan` in `src/visualization.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996504a-28fc-48f1-8094-94d9a6bd58f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_photo_plan(computed_plan)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff5833-f107-4332-974c-b9294e02a1da",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{TODO: }}$ Perform the following experiments (and any other you can think of) where we change just one parameter of the input camera/dataset specification and observe the change in the output plan. \n",
    "\n",
    "1. Change overlap and confirm it affects the consecutive images\n",
    "2. Change sidelap and confirm it does not affect the consecutive images\n",
    "3. Change the height of the scan and document the affect on scan plans\n",
    "4. Change exposure time\n",
    "\n",
    "Each experiment should specify: \n",
    "1. Input params you are changing\n",
    "2. Impact you observe\n",
    "3. explanation behind the change in output (intuition based or a text explanation is preffered over using equations)\n",
    "4. Practical implication of the correlation: how can I drone pilot use this result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe090a-b696-4800-8373-6f4b3be02a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_ = copy.deepcopy(camera_x10)\n",
    "dataset_spec_ = copy.deepcopy(dataset_spec)\n",
    "\n",
    "dataset_spec_.exposure_time_ms = 1000\n",
    "\n",
    "print(camera_, dataset_spec_)\n",
    "\n",
    "fig = plot_photo_plan(generate_photo_plan_on_grid(camera_, dataset_spec_))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
